{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jvret\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\jvret\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\jvret\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\jvret\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\jvret\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\jvret\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\jvret\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\jvret\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\jvret\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\jvret\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\jvret\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\jvret\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\jvret\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# Initialization\n",
    "rng = np.random\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "batch_size = 100\n",
    "vocab_size = 10000\n",
    "n_hidden_1 = 256\n",
    "n_hidden_2 = 256\n",
    "n_embedding = 256\n",
    "positive_samples = 4\n",
    "\n",
    "data_index = 0\n",
    "window_size = 25\n",
    "half_window = round((window_size - 1) / 2)\n",
    "num_skips = 2\n",
    "words_per_batch = round(batch_size / num_skips)\n",
    "\n",
    "\n",
    "# Model itself\n",
    "layer_input = tf.placeholder(\"float\", [batch_size, vocab_size], \"layer_input\")\n",
    "layer_output_nums = tf.placeholder(tf.int32, [batch_size, positive_samples], \"layer_output\")\n",
    "\n",
    "w_l1 = tf.Variable(tf.random_normal([vocab_size, n_hidden_1]))\n",
    "w_l1_bias = tf.Variable(tf.random_normal([n_hidden_1]))\n",
    "layer_1_sum = tf.add(tf.matmul(layer_input, w_l1), w_l1_bias)\n",
    "layer_1 = tf.nn.relu(layer_1_sum)\n",
    "\n",
    "\n",
    "w_l2 = tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]))\n",
    "w_l2_bias = tf.Variable(tf.random_normal([n_hidden_2]))\n",
    "layer_2_sum = tf.add(tf.matmul(layer_1, w_l2), w_l2_bias)\n",
    "\n",
    "w_out = tf.Variable(tf.random_normal([vocab_size, n_hidden_2]))\n",
    "w_out_bias = tf.Variable(tf.random_normal([vocab_size]))\n",
    "\n",
    "loss = tf.nn.sampled_softmax_loss(w_out, w_out_bias, layer_output_nums, layer_2_sum, \n",
    "                                  num_sampled = batch_size, num_classes = vocab_size, num_true = positive_samples)\n",
    "cost = tf.reduce_mean(loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "train_inputs = tf.placeholder(tf.float64, shape=[batch_size, vocab_size], name = \"train_inputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jvret\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'decode' is an invalid keyword argument for open()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-c8e26ab574ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;31m## get words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m \u001b[0mtrain_words\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mget_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_loc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train.pair_tok.tsv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-c8e26ab574ad>\u001b[0m in \u001b[0;36mget_words\u001b[1;34m(file_loc)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_loc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m#     lines = open(filename).read().decode('utf8').strip().split('\\n')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_loc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'decode' is an invalid keyword argument for open()"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# with open('train.pair_tok.tsv', encoding='utf-8') as f:\n",
    "#     sentences = csv.reader(f)\n",
    "#     words=[]\n",
    "#     for sentence in sentences:\n",
    "#         tokenized_sents = [word_tokenize(i) for i in sentence]\n",
    "#         for token in tokenized_sents:\n",
    "#             for each in token:\n",
    "#                 lower_text = each.lower()\n",
    "#                 punc = string.punctuation\n",
    "#                 punc = punc.replace('-','')\n",
    "#                 punc = punc.replace('\\'','')\n",
    "#                 punc = list(punc)\n",
    "#                 for p in punc:\n",
    "#                     lower_text = lower_text.replace(p,' ')\n",
    "#                 lower_text = lower_text.replace('-',' ')\n",
    "#                 words.append(lower_text)\n",
    "\n",
    "def get_words(file_loc):\n",
    "    with open(file_loc, encoding='utf-8') as f:\n",
    "        sentences = csv.reader(f)\n",
    "        words=[]\n",
    "        for sentence in sentences:\n",
    "            tokenized_sents = [word_tokenize(i) for i in sentence]\n",
    "            for token in tokenized_sents:\n",
    "                for each in token:\n",
    "                    lower_text = each.lower()\n",
    "                    punc = string.punctuation\n",
    "                    punc = punc.replace('-','')\n",
    "                    punc = punc.replace('\\'','')\n",
    "                    punc = list(punc)\n",
    "                    for p in punc:\n",
    "                        lower_text = lower_text.replace(p,' ')\n",
    "                    lower_text = lower_text.replace('-',' ')\n",
    "                    words.append(lower_text)\n",
    "        return words\n",
    "\n",
    "## get words\n",
    "train_words= get_words(file_loc='train.pair_tok.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 55143/55143 [00:00<00:00, 161330.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sod', 'ify', 'ag#', 'rdo', 'eve', 'pie', 'amd', 'ndf', 'ous', 'rew', 'lk#', 'sel', 'udy', 'sid', 'gui', 'eeb', 'ht#', 'los', 'sai', 'ick', 'ezo', 'mus', 'wel', 'nuc', 'lag', 'rif', 'uin', 'ma#', '#ef', 'wee', '#gh', 'wir', 'hot', 'ede', 'rio', 'mum', 'alb', 'agm', 'eet', 'tau', '#zi', 'you', 'jay', 'onb', 'wen', 'ths', 'yin', 'ars', 'id#', 'tco', 'hon', 'blo', 'lsv', 'eit', 'aci', 'sym', '#me', 'exa', 'cco', 'elt', 'lum', 'fir', 'hat', 'pio', 'asu', '98#', '#lo', '#gu', 'osc', 'nia', 'ubl', 'pai', 'did', '#eg', 'ool', 'inl', 'dfh', 'sui', 'cab', 'hoo', '#co', 'ey#', 'ege', 'onm', 'gn#', 'mag', 'tax', 'nci', '3th', 'teo', 'dru', 'ueg', 'eam', 'rta', 'plo', 'oym', '#ep', 'omy', 'ape', '08#', 'oz#', 'urk', 'anu', 'ary', 'lke', 'opt', 'oy#', 'mat', 'pti', 'mbo', 'dol', '#te', 'nob', '#cy', 'quo', 'zoo', 'gmt', '#at', 'led', 'its', 'til', 'arp', 'rcu', 'ish', 'xch', 'inj', '###', 'uma', 'xce', 'dje', 'iel', 'bbe', 'jec', 'yde', 'rne', '#ly', '#gm', 'het', 'gh#', 'sol', '#ph', 'cad', '00s', 'ema', 'rar', 'alu', 'rap', 'dis', 'ard', 'gur', 'ini', 'pt#', 'ubu', 'ves', 'ok#', 'ck1', 'oth', '#bo', 'sba', 'ilh', 'to#', 'da#', 'eni', 'uys', 'oss', 'cre', 'low', 'cy#', '#aw', 'mec', 'lde', 'sts', 'kos', '819', 'phy', '01#', 'edi', 'hol', 'abb', 'bas', 'ine', 'aho', 'igi', '#ce', '#pa', '#we', '#ab', 'bib', 'ono', 'uly', '#ov', 'dar', 'ham', 'ctl', '#th', 'omp', 'tn#', 'ura', 'ion', 'vem', 'mos', 'ly#', 'mon', 'axe', 'lid', 'isp', '#nb', 'am#', 'nom', 'ran', 'wag', 'kup', 'uy#', 'see', 'sco', 'usa', 'mb#', 'wor', 'dr#', 'gie', 'rde', 'lpe', 'umc', 'tea', 'bou', 'air', 'cop', 'sew', 'ene', '#ye', 'lik', 'tal', 'urt', 'emi', 'lli', '#ev', 'evo', 'dju', 'lav', 'une', '#1#', 'imo', 'nce', 'bo#', '#ic', 'cee', 'mi#', 'ata', 'cri', 'rgu', 'dcu', 'dre', 'ial', 'uak', 'urg', 'iu#', 'rei', '#ts', 'utc', 'spy', 'pir', 'ada', 'le#', 'ow#', '30#', 'mtv', 'fad', 'el#', 'io#', 'oll', 'unk', 'nig', 'ute', 'aws', 'roe', 'ian', 'poe', 'onl', 'anc', 'ics', 'des', 'len', 'phi', '09#', 'clo', 'ook', 'ex#', 'ker', 'ore', 'kil', 'gth', 'gor', 'an#', 'eor', 'upi', 'red', 'fte', 'go#', 'eta', 'rou', 'par', 'ces', 'rog', 'een', 'wne', 'ry#', 'in#', 'ues', 'rie', 'omb', 'rve', '#2p', 'lay', 'efa', 'nau', 'nne', 'ssi', 'lys', 'ket', 'deg', '#an', 'ayn', '922', 'ees', 'ap#', 'val', '#sz', '#mo', '#2#', '#tn', 'dyl', 'om#', 'not', 'lhe', 'pet', 'cia', 'ea#', 'cea', 'ofe', 'nag', 'orm', 'agi', '#sp', 'ue#', 'ome', 'son', 'are', 'rpo', 'asy', 'egn', 'reh', 'atu', 'rfe', 'una', 'on#', 'udg', 'fri', 'sph', 'hap', 'llv', 'coh', 'reg', 'mel', '#oz', '#ut', '#df', 'wik', 'osi', 'ark', 'ike', 'ayl', 'onc', 'lio', 'eyh', 'som', 'ghs', 'ros', 'nok', 'gy#', 'mes', 'imi', 'tut', 'wit', 'rre', '013', 'teg', 'aoh', 'rui', '#30', 'rto', 'cov', 'tat', 'use', 'dam', 'raz', 'oar', 'amo', 'ngd', 'lam', 'cs#', '#if', 'bel', 'ton', 'dsh', 'lt#', 'amy', 'pes', 'tod', '#fo', 'mem', 'uer', 'eak', 'ais', 'es#', 'ril', 'clu', 'mpu', 'ott', 'idd', 'col', 'lyn', '#os', 'dlo', 'lar', 'joi', 'mpe', 'rsi', '#az', 'let', '#fu', 'tas', 'mad', 'ynh', 'ofi', 'cot', 'nou', 'was', 'aij', 'hav', 'ose', 'hio', 'oni', 'but', 'ldc', 'ule', 'yor', 'hef', 'ct#', 'ppo', 'eti', 'pis', 'wiv', 'hei', 'uis', 'lla', 'fer', 'abo', 'bsu', 'ank', 'sin', '#ch', 'mea', 'eri', 'cho', 'hil', 'bod', '23#', '#st', 'seu', 'dhe', 'tma', 'rot', 'aub', 'rey', 'cog', 'eag', 'oup', 'ndo', 'pin', 'nuf', 'gus', 'fs#', '#wo', 'lub', 'ldw', 'sio', 'ein', 'r2#', '#su', 'doi', 'unc', 'atr', 'vma', 'dri', 'dap', 'me#', 'mui', '#wh', 'ni#', 'sir', 'rom', 'gin', 'hs#', 'bin', 'irt', 'xt#', 'uce', '886', 'aut', 'gen', 'ger', 'spu', 'hle', 'be#', 'ab#', 'gto', 'eto', 'ama', 'erc', 'lly', '#el', 'sim', 'gis', 'ngt', 'tri', 'epi', 'eir', 'aby', 'lfr', 'isl', 'cau', '00#', 'feb', 'diu', 'sat', 'uen', 'mis', 'ed#', '#br', 'cur', 'gdo', 'dve', 'aso', 'ori', 'nst', 'ire', 'hts', 'eiz', '#ww', 'ite', 'awo', 'off', '0s#', 'aco', 'cum', 'bad', 'dom', '#ta', 'il#', 'dtr', '#ya', 'rce', 'rev', 'e1#', '#se', 'pat', 'rua', 'inv', 'bee', 'sdi', '#ty', '181', '#ns', 'fau', 'ble', 'lfe', 'suc', 'rpi', 'nin', '#ct', 'of#', 'aus', 'opu', 'aba', 'obe', 'nct', 'rai', 'bor', 'nyc', 'ebt', 'ak#', 'nit', 'lak', 'noo', 'ad#', 'que', 'pte', 'nve', 'ipl', 'bro', 'cks', 'cce', 'lot', '#cl', 'set', 'ske', 'aly', 'lit', 'emo', 'lab', 'jur', '#dn', '#si', 'uti', 'tar', 'min', 'ubd', 'ogy', 'rvi', 're#', 'vin', 'lme', 'ici', 'sho', 'tic', 'adv', 'gre', 'bus', '#as', 'upp', 'las', 'ilm', 'de#', 'mma', 'rbo', 'ttr', 'sea', 'thn', 'zon', 'she', 'sit', 'chn', '#le', 'tor', 'ron', 'cem', 'thr', 'ica', 'wan', 'rop', 'exc', 'rdi', 'gua', 'cob', 'sta', 'aps', 'sus', 'eup', 'eks', 'rea', 'vel', 'ui#', 'als', '#ou', 'ops', 'ras', 'fro', 'uro', 'bat', 'nvo', 'mil', 'zip', 'dog', 'rge', 'civ', 'ise', 'ngr', 't50', 'ked', 'ber', 'ga#', 'lis', 'ype', 'def', 'urc', 'eco', 'pli', 'lon', 'ip#', 'iva', 'fy#', 'cit', 'gat', 'acy', 'bol', 'zai', 'fan', 'dif', 'ibi', '#sm', 'oga', 'oho', 'rul', 'cin', 'ne#', 'hey', 'uyn', 'med', 'nl#', 'ude', 'ywa', '#mu', 'rol', 'eur', 'ibl', 'men', 'deo', 'hee', 'abl', 'tc#', 'kip', '#pe', 'nod', 'omi', 'oan', '#20', '#16', 'eda', 'cer', 'rms', 'ame', 'opi', 'spe', 'usb', 'fet', 'ipa', 'gum', 'bam', 'awa', 'boo', 'tim', 'mpi', 'eun', 'nei', 'els', 'cui', 'ulp', 'pou', '#pi', '11#', 'eng', 'fiv', 'wme', '102', 'mot', 'mic', 'ndi', 'tho', '101', 'fea', 'ngc', 'ope', '#op', 'pso', 'hor', 'her', 'vam', '200', 'etr', 'fac', '86#', 'tit', 'ys#', 'adi', 'dp#', 'oop', 'cup', 'via', 'cul', 'ddi', 'fic', 'nea', 'rag', 'ate', 'ayi', 'tir', 'way', 'kon', 'rco', 'ltu', 'ium', 'rok', 'ign', 'irl', 'iss', 'irp', 'tme', 'uad', 'ipi', '#be', 'fly', 'oca', '#6#', 'kie', 'urd', 'whe', 'nan', 'ldr', 'cam', 'mid', 'udi', 'hic', 'oli', 'inn', 'ved', 'kin', 'ver', 'req', 'api', 'nts', 'mom', 'tom', 'pol', 'cvg', 'orc', 'ant', 'pst', '#gy', 'lbu', 'pit', 'ack', 'nfa', 'ymn', 'fti', 'fig', 'vil', 'pok', 'yno', '#d#', 'op#', 'luk', '#do', 'mna', 'oac', 'rt#', 'any', 'obb', 'rel', 'oce', 'tos', 'tta', 'sto', 'ob#', 'ala', 'nfi', 'law', 'od#', 'ney', '#10', 'wea', 'kid', 'apt', 'str', 'esp', 'liz', 'fes', 'sku', 'hom', 'ght', 'nh#', 'irm', 'ch#', 'dge', 'md#', 'elp', 'ast', 'eme', 'oph', 'x20', '112', 'bei', 'swo', '#bi', 'sys', 'stc', 'tem', 'rlf', 'his', '#li', '#ap', 'pur', 'tto', '#or', 'lev', 'amp', 'squ', 'coa', 'ali', 'jou', 'nur', '#pr', 'aug', 'qui', 'wal', 'sce', 'jil', 'eci', 'dmi', 'opo', 'mig', 'mod', 'arg', 'loy', 'tur', 'wls', 'run', 'tba', '02#', '#wi', 'ece', 'eec', '009', 'car', 'nni', '#vm', 'ess', 'rld', '196', '#s#', 'com', 'hum', 'ttl', 'war', '#tw', 'equ', 'arl', 'why', 'ili', 'mbi', '0fi', '964', 'lco', 'gov', 'oti', 'isu', 'ck#', 'rti', 'leb', 'dbo', 'opp', 'jer', 'oal', 'fif', 'stm', 'fas', 'rri', 'nno', 'ie#', 'sab', 'hin', 'unn', 'sso', 'bid', 'him', 'roy', 'fli', 'enb', 'eac', 'non', 'sen', 'cte', 'wom', 'fl#', '#sc', 'won', 'lat', 'ner', '#je', 'pec', 'ln#', 'mps', 'flu', 'nal', 'ich', 'tai', 'nta', 'uem', 'ole', 'gac', 'ibr', 'bie', 'ele', 'll#', 'vac', 'imu', 'ny#', 'pas', 'sfe', 'nsp', 'yed', 'tow', 'hou', 'ups', '#go', 'soc', 'edl', 'bir', '#13', 'cha', 'rad', 'sw#', 'usl', 'nd#', 'niv', 'put', 'eru', 'oka', 'ugh', 'inu', 'ibs', 'rry', 'maj', 'bje', 'gel', '#of', '#mi', 'atl', '2pm', 'ogt', 'ado', 'em#', 'ler', '#tu', 'bos', 'ins', 'dc#', 'wn#', '#fi', 'ond', 'nto', 'rod', 'ets', '#la', 'amb', 'ley', 'ief', 'eiv', 'exh', 'tv#', 'nds', 'ifi', 'fed', 'mai', 'yto', 'deb', 'rig', '#es', 'nha', 'wer', 'beg', 'gra', 'cil', 'vg#', 'ryd', 'sib', 'ewi', 'sic', 'rfo', 'ng#', 'enu', 'irs', '#8a', 'who', '012', 'tel', 'eap', 'ret', 'voi', 'odu', 'ff#', '#ow', 'jac', 'pre', 'uke', 'elm', '#sy', 'tre', 'ach', '#sa', 'uel', 've#', '#gn', '8am', 'arr', 'iki', 'az#', 'ep#', 'eav', 'alt', 'oye', 'etw', 'sly', '#ra', '#ki', 'iff', 'rme', 'nfe', 'bbr', 'ke#', 'ez#', 'isk', 'awn', 'rks', 'ter', 'ox#', 'ogz', '#cv', 'hoc', 'bec', 'ek#', 'esc', 'yli', 'sou', 'obl', 'tou', 'hre', 'ell', '#ul', 'pan', 'rma', 'mou', 'my#', 'oma', 'orh', 'buf', 'hob', 'oks', 'ps#', 'ans', 'pez', 'tro', 'us#', 'scr', 'ro#', 'sza', 'roc', 'nva', 'ict', '#ca', 'add', 'gul', 'mbe', 'ded', 'orn', 'sas', 'res', '#ed', '#di', 'pri', 'ri#', 'duc', 'iti', 'upa', 'nli', 'xpl', 'tra', 'lud', 'ig#', 'up#', 'dra', 'vas', '#ar', 'ymb', '#fe', 'ual', 'ggi', 'ake', '#va', 'sha', 'cei', 'nel', 'sam', 'oot', 'umb', 'ww2', 'riz', 'neu', 'afr', 'nsh', 'lyr', 'bre', 'wes', 'ara', '#hu', 'irc', 'ev#', 'hwe', 'enr', 'hri', 'dvi', 'lai', 'sch', 'own', '#ru', 'ort', 'gch', 'tex', '65#', 'roo', '2nd', 'cah', 'ive', 'ito', 'mak', 'er2', '#dc', 'aft', 'ub#', 'efe', 'tup', 'tad', 'lex', 'oon', 'lea', 'lln', 'key', 'ear', 'gai', 'zy#', 'had', 'ote', '#is', 'ben', 'wip', 'raf', 'vid', 'wnf', 'aks', 'pay', 'thq', 'ove', 'ppe', 'sci', '#ko', 'fit', 'eig', 'ssa', 'uns', 'rno', 'yal', 'lee', '#dy', 'ov#', 'exu', 'dic', 'nju', 'nex', 'hea', 'row', 'ste', 'din', 'ia#', 'ity', 't2#', 'ern', 'igg', 'hie', 'ech', 'uot', 'ld#', 'epl', 'agn', 'inf', 'ert', 'ega', '#hi', '180', '#fa', 'ug#', '#fr', 'xas', 'fie', 'lm#', 'ily', 'say', 'est', 'rga', 'arv', 'ass', 'rov', 'iza', 'lty', 'zed', 'nty', '#af', '#eq', 'pop', 'ipe', '#my', 'ien', 'eff', 'cc#', 'he#', 'oam', 'gan', 'ora', '#em', 'gla', 'usn', 'ist', 'muc', 'ume', 'aff', 'nth', 'taf', 'lie', 'rls', 'kyw', 'jud', '#4#', 'acc', 'tld', '199', 'ot#', 'gaz', 'ann', '998', 'edb', 'as#', 'obj', 'pm#', 'sla', 'ink', 'zar', 'epr', '64#', 'alk', 'hte', 'nbu', '#by', '#po', 'taw', 'xac', 'thl', 'rk#', 'nt5', 'nac', 'fc#', 'wha', 'orl', '#au', 'ns#', 'uat', 'nki', '#e3', 'owl', 'age', 'rip', 'fin', 'lym', 'gal', '#ka', 'die', 'te#', 'giv', 'gog', 'eca', 'hel', 'ntl', 'uk#', 'der', 'pha', 'alp', 'ana', 'nee', 'evi', 'dos', '#uk', 'nme', 'rab', 'iar', 'yra', 'ock', 'sse', 'ca#', 'whi', 'wry', 'nde', 'yea', '#pp', '#ri', 'ghb', 'cep', 'ddy', 'rub', 'pea', 'bil', 'dia', 'ypt', 'fun', 'wat', 'spr', 'gue', 'dei', 'ete', 'ogn', 'cen', 'ucl', 'emy', 'rty', 'ect', 'urn', '12#', 'dby', 'yla', 'voc', '#ji', 'ti#', 'eba', 'adg', 'rro', '#ir', '#nu', 'eer', 'um#', 'sig', 'uba', 'uoi', 'sca', '#kt', 'mci', 'ula', 'ts#', 'too', 'omm', 'ade', 'olo', 'lve', 'bit', 'int', 'ze#', 'lia', 'bar', 'og#', 'ven', 'sex', 'iot', 'ajo', 'wil', 'ufa', 'ks#', 'gro', 'net', 'doe', 'uee', '#ro', '#tv', 'uri', 'vat', '#ea', 'aur', 'sua', 'scu', 'fam', 'erg', 'ws#', 'ong', 'nge', 'and', 'oly', 'onf', 'wl#', '#tl', 'or#', 'can', 'sue', 'now', 'aca', '#mt', '#bu', 'lop', 'zil', 'se#', 'typ', 'aul', 'aph', 'za#', 'ral', 'pro', 'sec', 'vol', 'ots', 'day', 'ef#', 'nfo', 'tak', 'bri', 'ryi', 'lut', 'agr', 'ten', 'oda', 'ony', 'cas', 'rec', 'ddr', 'ntr', 'oey', '#ge', 'ncl', 'twe', 'wro', 'st#', 'ent', 'mas', 'sac', 'dio', 'eog', 'ses', 'hig', 'uit', 'otl', 'spi', 'upr', 'ild', 'ave', 'has', 'ria', 'ba#', '#ja', 'yss', 'iz#', 'foo', 'iqu', 'pla', '#x2', '#kl', 'cir', 'ota', 'izo', 'aig', 'pon', 'hal', '#5#', 'god', 'ilb', 'ar#', 'cti', 'w2#', 'th#', 'uni', 'att', '#ve', 'out', 'mov', 'yme', 'box', 'rtw', 'bst', 'anr', 'rav', 'ful', 'ams', 'zoe', 'asi', 'ref', 'lec', 'rch', 'bum', 'ory', 'apo', 'oba', 'rak', '#u#', 'ats', 'owr', 'ere', 'ped', 'adu', 'ss#', 'ilt', 'rho', 'swe', 'ase', 'eed', 'nas', '#un', 'haf', 'mit', 'ouc', 'ige', 'hoi', 'yc#', 'cra', 'ery', 'cru', '#24', 'alm', '#gd', 'wsp', 'py#', 'lom', 'fid', 'huy', 'rib', 'ods', 'del', 'lte', 'ide', 'hme', 'got', 'isc', '008', 'may', 'is#', 'thw', '#eu', 'ppl', '#19', '#on', 'suf', 'oya', 'ury', 'nco', 'cie', 'bui', 'odl', '#ag', 'lor', 'ngu', 'ize', 'dyn', 'os#', 'oo#', 'egu', '#ju', 'sup', 'pho', 'ile', 'rso', 'hsa', 'wif', '#sl', 'ila', 'gri', 'bef', 'iam', 'kfa', 'uar', 'bet', 'eal', 'odi', 'ram', 'kno', 'ngl', 'tam', '#gl', 'rit', 'nus', '24#', 'gir', 'hen', 'enh', 'ina', 'daw', 'ife', 'ati', 'ffi', 'tne', '#sw', 'mur', '10f', 'nly', 'onv', 'win', 'tif', 'rmy', 'dau', '#0#', 'boy', 'lap', 'im#', '#wr', '#ha', 'guo', 'man', 'ood', 'ppp', 'org', 'ued', 'vad', '04#', 'ktr', 'ang', '#rc', 'web', 'ntm', 'rim', 'uci', 'pub', 'dal', 'kor', 'ord', 'met', 'era', 'bli', 'pil', 'icl', '#to', 'ody', 'idi', 'rke', 'how', 'arc', 'cor', 'ils', 'uld', '19#', 'eop', '#cr', 'pto', '#js', 'yna', 'tis', 'pap', 'aw#', 'num', 'rs#', 'oic', 'rmo', '#up', 'acl', 'tie', 'elo', '188', 'bes', 'gni', 'poi', 'ges', 'oun', 'rds', 'efo', 'fle', 'ima', 'kai', 'mal', 'tle', 'hai', 'sc#', 'iro', '123', 'isi', 'iag', 'nue', 'sey', 'edu', 'ena', 'ego', 'bbi', '#wa', 'hod', '#ti', 'nic', 'ew#', 'two', 'sun', 'sal', 'ra#', '#kn', 'fel', 'etb', 'iri', 'ode', 'fec', 'fre', 'enc', '#al', 'oh#', 'nt#', 'icr', 'usp', 'ymp', 'ism', 'rem', 'itu', '#cc', 'isa', '#us', 'ol#', 'dop', 'sak', 'xhi', 'sif', '#sn', 'spl', 'ost', '#py', 'ww1', 'mmo', '#sh', 'twi', 'lem', '#il', 'by#', 'lms', 'erm', 'jus', '#ad', 'dub', 'iat', 'pul', 'tua', 'ren', 'rst', 'xis', 'sis', 'pta', 'gm#', 'ani', 'niz', 'ccu', 'eat', 'geo', 'kna', 'ecu', 'bow', 'san', 'wns', 'co#', 'mpl', 'ung', 'atc', 'ona', 'cis', 'lan', 'dec', 'iod', 'xtr', 'nsw', 'aro', 'ou#', 'fol', 'bdi', 'guy', 'nis', 'ric', 'ged', 'cus', 'fou', 'edg', '10#', 'ice', 'afl', 'peo', 'ul#', 'phe', 'yle', 'ied', 'dle', 'ffy', 'lse', 'vis', 'log', 'rah', 'nen', 'nch', 'oln', 'moo', '#im', '#04', 'oke', 'ums', 'do#', '#ni', 'pot', 'urp', 'pos', 'nsi', '#dr', 'la#', 'adh', 'sko', 'oad', '#ob', 'zes', 'oul', 'bra', 'ngh', 'nle', 'goa', 'acr', 'ath', 'hib', 'ifa', 'emp', '#aa', 'egy', 'cto', 'yer', 'osp', 'gns', 'xes', 'dir', 'tag', 'pme', 'ush', 'exp', 'ill', 'asc', 'nt2', 'wav', 'spo', 'bon', 'nar', '#sq', 'sh#', 'lal', 'loc', 'pun', 'rus', 'tew', 'ka#', '#qu', 'uff', 'chy', 'xpo', 'hy#', 'ols', 'mt#', 'nk#', 'rum', 'hni', 'boa', 'und', 'tun', 'esa', 'ur#', 'opm', 'mba', 'art', '192', 'lif', 'gas', 'ers', 'tch', 'yri', 'azy', 'hag', 'rek', 'imp', 'eso', 'zat', 'ree', '#tr', 'ida', 'rup', 'ano', 'er#', 'opl', 'thi', 'en#', 'sm#', 'rsh', 'ohi', 'hsc', 'emb', 'aku', 'ace', 'ese', 'ir#', 'dow', 'ita', 'efs', 'rep', 'faw', 'eli', 'egr', 'mor', '#av', 'vot', 'ely', 'dun', 'ghe', 'ont', 'hop', 'pe#', 'ato', 'tha', 'gar', 'vie', 'shm', 'ebr', 'upe', 'mpr', 'cli', '#ec', 'eek', '#da', '011', '#fc', 'end', '#ci', 'gs#', 'dep', 'ytv', 'lpr', 'tap', 'hip', 'ett', 'pyr', 'jsc', 'oid', 'ugu', 'bal', 'gio', '#ga', 'one', 'llo', 'usc', 'aki', 'eau', 'jan', 'dus', 'nim', 'sky', 'tio', 'hro', 'ibu', 'ssu', 'tly', 'stu', 'liv', 'yee', 'arm', 'adj', '#oh', 'jiu', 'mpa', '#cu', 'tan', 'pus', 'erb', 'fhu', 'rni', 'ffe', 'hos', 'owa', 'erf', 'eke', 'pel', 'iso', 'act', 'uto', 'uiv', 'oin', 'ews', 'rts', '#fl', 'the', 'ic#', 'tay', 'cle', '#in', 'chu', 'hes', 'al#', 'hbo', 'jai', 'igu', 'ilo', 'hem', '#it', 'abr', 'nti', 'di#', 'riu', 'egi', 'ban', 'bia', 'tab', '#ke', 'ulf', 'tte', 'kes', 'sor', 'lin', 'eds', '#oc', 'don', 'ero', 'sma', 'ugs', 'dem', 'fla', 'mer', 'con', 'ho#', 'erv', 'hqu', 'inb', 'ruc', 'dua', 'nte', 'nor', 'ahs', 'gna', '22#', 'our', 'ted', 'vio', 'iou', 'unt', '010', 'les', 'umm', 'ail', '#ai', 'dan', '#ac', 'ki#', 'tee', 'hus', 'hur', 'ple', 'orp', 'eid', 'rte', 'bed', 'w1#', 'un#', 'blu', '#18', 'upt', 'fe#', 'kni', 'ris', 'nb#', 'seb', 'tla', 'ut#', 'rac', 'olv', 'ubj', 'ta#', 'olf', 'nda', '#no', '#3#', 'akf', 'nut', 'dna', 'ime', 'fil', 'eld', 'dev', 'wo#', 'aha', 'mpo', 'xts', 'tin', 'lue', 'gad', 'inc', '#he', 'ppi', 'pid', 'gma', 'otb', 'dit', 'bsi', 'ifo', '166', 'ndt', 'ayo', 'ds#', 'wiz', 'ova', 'yne', 'apa', 'ken', 'den', 'ty#', 'rue', 'cal', 'rob', '#er', 'dad', 'nfl', 'ldi', 'uch', '#am', 'k11', 'adq', 'iet', '#2n', '#a#', 'ind', 'nna', 'lov', 'pra', 'shi', 'niq', 'ows', 'sti', 'lne', 'rth', 'luc', 'owe', 'ubb', 'snl', 'wri', 'cou', '#ol', 'adc', 'lpt', 'wie', 'rof', 'mez', 'ubs', 'pic', 'gne', 'wwe', 'uct', 'ee#', 'nat', 'cod', '#ma', 'rcr', 'sub', '#lu', 'pee', 'esi', 'che', 'pow', '#id', 'leg', 'jen', 'oci', 'svi', '201', 'nsu', '#jr', 'flo', 'if#', '#bl', 'igo', 'sno', 'itt', 'dea', 'ngs', 'ndu', '50#', 'nvi', 'gam', 'rmi', 'lic', 'rd#', 'fen', 'ray', 'xpe', 'spa', 'cel', 'bs#', 'fee', 'lti', 'abe', 'chr', 'cec', 'nga', 'rtn', 'lbo', 'rra', 'nso', 'mar', 'cke', '665', 'agu', 'ud#', 'ctu', 'xua', 'new', 'mme', 'ult', 'sei', 'ycl', 'erp', '#ho', 'dur', '#ot', 'nif', 'mab', '#sk', 'oug', 'hab', 'oat', 'per', 'gyp', 'uag', 'anl', 'ors', 'wnt', 'hud', 'rnm', 'ing', 'dy#', 'ons', 'lpa', 'yfr', 'hit', '#ba', '#ne', '#od', 'rtr', 'iom', '#na', 'ced', 'nov', '#gr', 'ndb', 'rlo', 'ela', 'sum', 'alc', 'stl', 'rao', 'ror', 'exi', 'tyl', 'sul', 'rli', 'fal', 'oki', '#vi', 'osb', '#de', '003', 'bru', 'olu', 'div', 'rug', '#pl', 'for', 'bic', '#et', 'tes', 'ebs', 'all', 'urr', 'rid', 'rby', 'ask', 'cyc', 'gza', 'uca', 'lls', 'qua', 'ewm', 'ay#', 'tbr', 'mow', 'yst', 'eyt', 'bul', 'rm#', 'hed', 'ft#', 'loo', 'occ', 'ibe', 'ogr', 'job', 'sna', 'lib', '#jo', 'por', 'djo', 'yah', 'koh', 'tia', 'tec', 'jor', 'top', 'eou', 'bab', 'sty', 'dfo', 'itl', 'erl', 'ski', 'gym', 'ucc', 'pen', 'yho', 'dwo', '#gi', 'tyt', '#so', 'tiv', 'cts', 'eha', 'wou', 'bt#', 'rpe', 'dca', 'heu', 'ems', 'ifu', 'bea', 'try', 'aye', 'nro', '#du', '800', '13t', 'hub', 'nad', 'cat', 'rly', 'olm', 'nke', 'bla', 'riv', '03#', '#i#', 'erd', 'ari', 'cip', 'ge#', 'pp#', 'env', 'ust', 'chi', 'obs', 'uil', 'igh', 'zin', 'rin', 'rn#', 'ls#', 'rat', 'ids', 'urf', 'oyf', 'ier', 'vic', 'ead', 'avi', 'jr#', 'nsf', 'ith', 'rph', 'tru', 'oro', 'app', 'uts', 'at#', 'vek', 'oet', 'ser', 'ivi', 'rse', 'fat', 'har', '#ny', 'ays', 'iev', 'na#', 'azi', '#en', 'roa', 'ure', 'usi', 'lvi', 'toc', 'gha', 'nes', 'ale', 'heo', 'ain', 'mup', '#x#', 'dqu', 'eum', 'ovi', 'ncy', 'oes', 'e3#', '#zo', 'aar', 'xte', 'gdp', 'isd', 'amu', '13#', 'cap', 'arb', 'eas', 'loa', 'iol', 'sa#', 'rer', 'old', 'get', 'glo', 'big', 'gol', '#e1', 'tti', 'dat', '#re', 'nio', '100', 'cro', 'ami', 'nba', 'vir', 'adm', 'ce#', 'ane', 'bur', 'uck', 'tud', 'uth', 'abu', 'xan', '#ex', 'aid', 'eth', 'nre', 'sy#', 'gig', 'pad', 'err', 'lig', 'nev', 'ept', 'klu', 'et#', 'nam', 'lac', 'ms#', 'ean', 'sed', '#pu', 'aty', 'aga', 'ckn', 'mac', 'abi', '#vo', 'ips', 'eno', 'sur', 'sau', 'han', 'ash', 'lf#', 'kud', '#yo', 'ies', 'map', 'ija', '#nf', 'enn', 'lle', 'cla', 'ext', 'buy', 'we#', 'ned', 'it#', 'bio', 'wed', 'fra', 'tsu', 'gag', 'lma', 'ork', 'rna'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "vocab_size = 10000\n",
    "\n",
    "def explode_word(word):\n",
    "    w = \"#\" + word + \"#\"\n",
    "    for i in range(0, len(w)-3+1):\n",
    "        yield w[i:i+3]\n",
    "\n",
    "count = Counter()\n",
    "for w in tqdm(train_words):\n",
    "    count.update(explode_word(w))\n",
    "\n",
    "count = [[\"###\", -1]] + count.most_common(vocab_size - 1)\n",
    "dictionary = defaultdict(lambda: 0)\n",
    "\n",
    "for i, k in enumerate(count):\n",
    "    dictionary[k[0]] = i\n",
    "\n",
    "known_trigrams = set(dictionary.keys())\n",
    "print(known_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = defaultdict(lambda: 0)\n",
    "\n",
    "def word2tri(word):\n",
    "    w = \"#\" + word + \"#\"\n",
    "    for i in range(0, len(w)-3+1):\n",
    "        yield dictionary[w[i:i+3]]\n",
    "\n",
    "def generate_batch():\n",
    "    global data_index\n",
    "    buffer_x = []\n",
    "    buffer_y = []\n",
    "    for _ in range(words_per_batch):\n",
    "        x = [0] * vocab_size\n",
    "        for i in word2tri(train_words[data_index + half_window]):\n",
    "            x[i] = 1\n",
    "        for _ in range(num_skips):\n",
    "            y = [0] * positive_samples\n",
    "            sample_word = rng.randint(-half_window, half_window - 1)\n",
    "            if half_window >= 0: \n",
    "                sample_word += 1 \n",
    "            sample_tris = list(word2tri(train_words[data_index + half_window + sample_word]))\n",
    "            for i, k in enumerate(rng.choice(sample_tris, positive_samples)):\n",
    "                y[i] = k\n",
    "            buffer_x.append(x)\n",
    "            buffer_y.append(y)\n",
    "        data_index = (data_index + 1) % (vocab_size - window_size)\n",
    "    return buffer_x, buffer_y\n",
    "## generate X, Y batch\n",
    "X, Y = generate_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 532.805236816\n",
      "Epoch: 0002 cost= 244.998672485\n",
      "Epoch: 0003 cost= 140.980514526\n",
      "Epoch: 0004 cost= 85.367652893\n",
      "Epoch: 0005 cost= 53.230663300\n",
      "Epoch: 0006 cost= 35.176074982\n",
      "Epoch: 0007 cost= 24.562246323\n",
      "Epoch: 0008 cost= 17.946758270\n",
      "Epoch: 0009 cost= 14.207668304\n",
      "Epoch: 0010 cost= 11.434528351\n",
      "Epoch: 0011 cost= 9.431775093\n",
      "Epoch: 0012 cost= 8.346570015\n",
      "Epoch: 0013 cost= 7.402019024\n",
      "Epoch: 0014 cost= 6.698512077\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        cc = []\n",
    "        for _ in range(round((vocab_size - window_size) / words_per_batch)):\n",
    "            batch_x, batch_y = generate_batch()\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={layer_input: batch_x, layer_output_nums: batch_y})\n",
    "            cc.append(c)\n",
    "        print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(np.mean(np.array(cc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
