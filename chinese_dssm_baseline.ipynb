{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test chinese dataset\n",
    "\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# Initialization\n",
    "rng = np.random\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "batch_size = 100\n",
    "vocab_size = 10000\n",
    "n_hidden_1 = 256\n",
    "n_hidden_2 = 256\n",
    "n_embedding = 256\n",
    "positive_samples = 4\n",
    "\n",
    "data_index = 0\n",
    "window_size = 25\n",
    "half_window = round((window_size - 1) / 2)\n",
    "num_skips = 2\n",
    "words_per_batch = round(batch_size / num_skips)\n",
    "\n",
    "\n",
    "# Model itself\n",
    "layer_input = tf.placeholder(\"float\", [batch_size, vocab_size], \"layer_input\")\n",
    "layer_output_nums = tf.placeholder(tf.int32, [batch_size, positive_samples], \"layer_output\")\n",
    "\n",
    "w_l1 = tf.Variable(tf.random_normal([vocab_size, n_hidden_1]))\n",
    "w_l1_bias = tf.Variable(tf.random_normal([n_hidden_1]))\n",
    "layer_1_sum = tf.add(tf.matmul(layer_input, w_l1), w_l1_bias)\n",
    "layer_1 = tf.nn.relu(layer_1_sum)\n",
    "\n",
    "\n",
    "w_l2 = tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]))\n",
    "w_l2_bias = tf.Variable(tf.random_normal([n_hidden_2]))\n",
    "layer_2_sum = tf.add(tf.matmul(layer_1, w_l2), w_l2_bias)\n",
    "\n",
    "w_out = tf.Variable(tf.random_normal([vocab_size, n_hidden_2]))\n",
    "w_out_bias = tf.Variable(tf.random_normal([vocab_size]))\n",
    "\n",
    "loss = tf.nn.sampled_softmax_loss(w_out, w_out_bias, layer_output_nums, layer_2_sum, \n",
    "                                  num_sampled = batch_size, num_classes = vocab_size, num_true = positive_samples)\n",
    "cost = tf.reduce_mean(loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "train_inputs = tf.placeholder(tf.float64, shape=[batch_size, vocab_size], name = \"train_inputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "## prepare data\n",
    "import json\n",
    "data_index = 0\n",
    "\n",
    "## get words\n",
    "def gen_word_set(file_path):\n",
    "    word_set = set()\n",
    "    with open(file_path, encoding='utf8') as f:\n",
    "        for line in f.readlines():\n",
    "            spline = line.strip().split('\\t')\n",
    "            if len(spline) < 4:\n",
    "                continue\n",
    "            prefix, query_pred, title, tag, label = spline\n",
    "            if label == '0':\n",
    "                continue\n",
    "            cur_arr = [prefix, title]\n",
    "            query_pred = json.loads(query_pred)\n",
    "            for w in prefix:\n",
    "                word_set.add(w)\n",
    "            for each in query_pred:\n",
    "                for w in each:\n",
    "                    word_set.add(w)\n",
    "    return list(word_set)\n",
    "\n",
    "##\n",
    "file_vali = 'oppo_round1_train_20180929_mini.txt'\n",
    "chi_wordset= gen_word_set(file_path=file_vali)\n",
    "print(type(chi_wordset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import numeric\n",
    "\n",
    "chi_wordset=[]\n",
    "for i in chi_wordset:\n",
    "    res = i.encode().decode('utf-8')\n",
    "    chi_wordset.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def explode_word(word):\n",
    "    dictionary = defaultdict(lambda: 0)\n",
    "    w = \"#\" + word + \"#\"\n",
    "    for i in range(0, len(w)-3+1):\n",
    "        yield w[i:i+3]\n",
    "##\n",
    "def word2tri(words):\n",
    "    count=Counter()\n",
    "    for w in tqdm(words):\n",
    "        count.update(explode_word(w))\n",
    "    count = [[\"###\", -1]] + count.most_common(vocab_size - 1)\n",
    "    dictionary = defaultdict(lambda: 0)\n",
    "    for key, val in enumerate(count):\n",
    "        dictionary[val[0]] = key\n",
    "    return list(dictionary.keys())\n",
    "\n",
    "tri_gram = word2tri(chi_wordset)\n",
    "print(type(tri_gram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import unicode_literals\n",
    "# from unicodedata import numeric\n",
    "\n",
    "# def to_integer(s):\n",
    "#     try:\n",
    "#         r = int(s)\n",
    "#     except ValueError:\n",
    "#         r = int(numeric(s))\n",
    "#     return r\n",
    "\n",
    "# print(to_integer('#åŸ”#'))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch():\n",
    "    global data_index\n",
    "    buffer_x = []\n",
    "    buffer_y = []\n",
    "    for _ in range(words_per_batch):\n",
    "        x = [0] * vocab_size\n",
    "        for _ in range(num_skips):\n",
    "            y = [0] * positive_samples\n",
    "            sample_word = rng.randint(-half_window, half_window - 1)\n",
    "            if half_window >= 0: \n",
    "                sample_word += 1 \n",
    "            words_ranges= word2tri(chi_wordset[data_index+half_window+ sample_word])\n",
    "            #sample_tris = list(word2tri(chi_wordset[data_index + half_window + sample_word]))\n",
    "            for i, k in enumerate(rng.choice(list(words_ranges), positive_samples)):\n",
    "                y[i] = k\n",
    "            buffer_x.append(x)\n",
    "            buffer_y.append(y)\n",
    "        data_index = (data_index + 1) % (vocab_size - window_size)\n",
    "    return buffer_x, buffer_y\n",
    "## generate X, Y batch\n",
    "# X, Y = generate_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-f28f1a66ecaf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mcc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mwords_per_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mlayer_input\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_output_nums\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mcc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-75-b07f26e627ed>\u001b[0m in \u001b[0;36mgenerate_batch\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhalf_window\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                 \u001b[0msample_word\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mwords_ranges\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mword2tri\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchi_wordset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata_index\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mhalf_window\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0msample_word\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[1;31m#sample_tris = list(word2tri(chi_wordset[data_index + half_window + sample_word]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords_ranges\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpositive_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        cc = []\n",
    "        for _ in range(round((vocab_size - window_size) / words_per_batch)):\n",
    "            batch_x, batch_y = generate_batch()\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={layer_input: batch_x, layer_output_nums: batch_y})\n",
    "            cc.append(c)\n",
    "        print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(np.mean(np.array(cc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
